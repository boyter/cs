This refined plan is designed for a code agent to implement the Structural Gravity Ranker. It focuses on the byte-level state masking and weighted ranking, while explicitly deferring the advanced complexity math to a later phase.
Implementation Plan: Structural Gravity Ranker (Phase 1)
Objective
Create a new ranking function, rankResultsStructural, that utilizes scc's state-map (the []byte slice where each entry maps to a byte-offset type) to apply weights to search hits.
IMPORTANT: This phase focuses strictly on state-based weighting (Code vs. Comment vs. String). We will be integrating the Complexity Gravity and Entropy Scaling in a separate, subsequent phase. Do not implement complexity-based boosting yet.

Structural Scoping Definitions
The ranker will use the following state types (aligned with scc constants) to categorize every match location.

Note that the exact state types come from `scc` itself

```go
// ByteType constants for per-byte content classification.
// When FileJob.ClassifyContent is true, CountStats populates
// FileJob.ContentByteType with one of these values per byte.
const (
ByteTypeBlank   byte = 0
ByteTypeCode    byte = 1
ByteTypeComment byte = 2
ByteTypeString  byte = 3
)
```

```
type StructuralConfig struct {
WeightCode    float64 // Influence multiplier for code hits
WeightComment float64 // Influence multiplier for comment hits
WeightString  float64 // Influence multiplier for string hits
}
```

an example of using scc to get this information

```go
  bts, _ := os.ReadFile("main.go")
  filejob := &processor.FileJob{
    Filename:        "main.go",
    Language:        "Go",
    Content:         bts,
    Bytes:           int64(len(bts)),
    ClassifyContent: true, // Enable per-byte classification
  }
  processor.CountStats(filejob)

  // ContentByteType has one entry per byte with values:
  //   processor.ByteTypeBlank   (0) - blank lines / leading whitespace
  //   processor.ByteTypeCode    (1) - code
  //   processor.ByteTypeComment (2) - comments (including docstrings)
  //   processor.ByteTypeString  (3) - string literals

  // Example: extract only code, replacing everything else with spaces
  codeOnly := filejob.FilterContentByType(processor.ByteTypeCode)
  fmt.Println(string(codeOnly))

  // Example: extract only comments
  commentsOnly := filejob.FilterContentByType(processor.ByteTypeComment)
  fmt.Println(string(commentsOnly))

  // Example: keep both code and strings, strip comments
  noComments := filejob.FilterContentByType(processor.ByteTypeCode, processor.ByteTypeString)
  fmt.Println(string(noComments))
```

For our purpose we can use the field below 

```Go
ContentByteType    []byte `json:"-"` // Per-byte classification, allocated by CountStats when ClassifyContent is true
```

Which has a 1 to 1 mapping of the content in `Content` of the Filejob returned, where every byte uses the consts 
`ByteTypeBlank ByteTypeCode ByteTypeComment ByteTypeString` to indicate if the content is string code or comment.

Since we know the location of each match, we can use this to apply the ranking rules.

The New Ranker: rankResultsStructural

This function introduces Layered Weighting. Instead of counting all matches equally, it calculates a Weighted Term Frequency (wtf) based on the state of the code at the match offset.
Logic Flow:
Iterate through each FileJob.
Access the StateMap: Use the []byte provided by scc for the current file.
Calculate wtf (Weighted Term Frequency):
For each word match, look up its byte offset in the StateMap.
Assign the weight based on the StructuralConfig (e.g., a hit in a comment might count as 0.1, while a hit in code counts as 1.0).
Execute BM25 Math: Plug the wtf into the standard BM25 formula.
Code Architecture (For the Code Agent)
func rankResultsStructural(results []*common.FileJob, config StructuralConfig, documentFrequencies map[string]int) {
// Standard BM25 Setup (k1, b, avgdl) remains the same

    for _, file := range results {
        var totalScore float64
        stateMap := file.StateMap // Provided by scc per byte

        for word, offsets := range file.MatchLocations {
            var weightedTf float64
            
            for _, offset := range offsets {
                // Determine the "Weight Layer" of this specific hit
                // O(1) lookup in the scc state map
                state := stateMap[offset]
                switch state {
                case StateCode:
                    weightedTf += config.WeightCode
                case StateComment:
                    weightedTf += config.WeightComment
                case StateString:
                    weightedTf += config.WeightString
                }
            }

            // Standard BM25 calculation, substituting 'rawCount' with 'weightedTf'
            idf := math.Log10(1 + float64(corpusCount)/float64(documentFrequencies[word]))
            // (Standard step1 / step2 calculation goes here)
            totalScore += calculatedWeight
        }

        file.Score = totalScore
    }
}

Future Scope (DO NOT IMPLEMENT YET)

We have a separate roadmap for the following features. We are doing these later:

Complexity Normalization: We will later integrate cyclomatic complexity as a multiplier for the final score.
Entropy Penalty: We will later implement file-size vs. signal-density scaling.
Language-Specific Stopwords: We will later implement dynamic idf dampening based on language-specific syntactic noise.

Expected Usage Examples
Intent: "Just show me where this is implemented"
--only-code \rightarrow WeightCode: 1.0, WeightComment: 0.0, WeightString: 0.0
Intent: "Search the docs for this term"
--only-comments \rightarrow WeightCode: 0.0, WeightComment: 1.0, WeightString: 0.0
Intent: "Search everything but prioritize logic"
--weighted \rightarrow WeightCode: 1.0, WeightComment: 0.2, WeightString: 0.5
